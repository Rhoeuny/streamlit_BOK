import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
import requests as req
from bs4 import BeautifulSoup as bs
import matplotlib.font_manager as fm

st.title('ğŸ“ˆë…¼ë¬¸ êµ¬í˜„ : ê¸ˆë¦¬ ì˜ˆì¸¡')
st.write("")

#ì‚¬ì´ë“œ ë°” ê¸°ëŠ¥ ì¶”ê°€
with st.sidebar:
    sdt = st.date_input('ì¡°íšŒ ì‹œì‘ì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”',pd.to_datetime('2005-06-01'))
    edt = st.date_input('ì¡°íšŒ ì¢…ë£Œì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”',pd.to_datetime('2020-01-01'))


# ì½œê¸ˆë¦¬,ë¬¸ì„œí†¤ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
final_data = pd.read_csv("hw_do/data/doc_tone_base_rate.csv")
# ë°ì´í„° í”„ë ˆì„ ë³€í™˜
df = pd.DataFrame(final_data)
df['date'] = pd.to_datetime(df['date'])
date_df = df[(df['date']>= pd.Timestamp(sdt)) & (df['date']<=pd.Timestamp(edt))]

# ì°¨íŠ¸
test = plt.figure(figsize=(10,7))
plt.rcParams["axes.unicode_minus"] = False
ax1 = date_df.doc_tone.plot(color='blue', grid=True, label='ê¸ˆí†µìœ„ ì˜ì‚¬ë¡ ì–´ì¡°')
ax2 = date_df.baserate.plot(color='red', grid=True, secondary_y=True, label='ê¸°ì¤€ê¸ˆë¦¬')
# ì™¼ìª½ yì¶• ë²”ìœ„ ì„¤ì •
ax1.set_ylim(-1, 0)
plt.show()



st.subheader('ê¸°ì¤€ê¸ˆë¦¬ì™€ ë¬¸ì„œí†¤ì˜ ìƒê´€ê´€ê³„')
# ê·¸ë˜í”„ ì¶œë ¥
st.pyplot(test)
# ë°ì´í„° í”„ë ˆì„ ì¶œë ¥(ì •ë ¬ ê¸°ëŠ¥ ì œê³µ)
st.dataframe(date_df, use_container_width = True)

st.write("-------------------------------------------------------------------")


st.subheader('ì‚°ì ë„ ë° ì¶”ì„¸ì„ ')
st.write('ì˜ì‚¬ë¡ ì–´ì¡°ì— ë”°ë¥¸ ê¸°ì¤€ê¸ˆë¦¬ ë¶„í¬')


import seaborn as sns
# seaborn ì„¤ì¹˜ í•„ìš”!
graph1 = plt.figure()
sns.regplot(x = "doc_tone", y = "baserate", data = date_df)
st.pyplot(graph1)

# st.caption('xì¶• : ì–´ì¡°  / yì¶• : ê¸°ì¤€ê¸ˆë¦¬')


# ì„ íƒ ë‚ ì§œ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰
def get_news_item(url) :
    res = req.get(url)
    soup = bs(res.text, "html.parser")
    date = soup.select_one("span.media_end_head_info_datestamp_time")["data-date-time"]
    title = soup.select_one("h2#title_area").text
    media = soup.select_one("a.media_end_head_top_logo > img")["title"]
    content = soup.select_one("div.newsct_article").text.replace("\n", "")
    return (date, title, media, content)

def get_news(sdt, edt) :
    page = 1
    result = []
    search = "ê¸ˆë¦¬"
    while True :
        if page == 11 :
            break
        start = (page - 1) * 10 + 1
        url = f"https://s.search.naver.com/p/newssearch/search.naver?de={edt}&ds={sdt}&eid=&field=0&force_original=&is_dts=0&is_sug_officeid=0&mynews=0&news_office_checked=&nlu_query=&nqx_theme=%7B%22theme%22%3A%7B%22main%22%3A%7B%22name%22%3A%22finance%22%7D%7D%7D&nso=%26nso%3Dso%3Add%2Cp%3Afrom{sdt.replace('.', '')}to{edt.replace('.', '')}%2Ca%3Aall&nx_and_query=&nx_search_hlquery=&nx_search_query=&nx_sub_query=&office_category=0&office_section_code=0&office_type=0&pd=3&photo=0&query={search}&query_original=&service_area=0&sort=0&spq=0&start={start}&where=news_tab_api&nso=so:dd,p:from{sdt.replace('.', '')}to{edt.replace('.', '')},a:all"
        res = req.get(url)
        doc = eval(res.text.replace("\n", ""))
        for lst in doc["contents"] :
            soup = bs(lst, "html.parser")
            a_tags = soup.select("div.info_group > a")
            if len(a_tags) == 2 :
                try :
                    result.append(get_news_item(a_tags[-1]["href"]))
                except Exception as e :
                    print("ì˜¤ë¥˜ : ", e)
        page += 1
    return pd.DataFrame(columns = ["date", "title", "media", "content"], data = result)

st.write("-------------------------------------------------------------------")

st.subheader("ë„¤ì´ë²„ ë‰´ìŠ¤")
st.write(f"{sdt.strftime('%Y-%m-%d')}ë¶€í„° {edt.strftime('%Y-%m-%d')}ê¹Œì§€ì˜ [ê¸ˆë¦¬] ë„¤ì´ë²„ ë‰´ìŠ¤ 'ê´€ë ¨ë„ìˆœ' ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.")

if sdt and edt :
    df_news = get_news(sdt.strftime("%Y%m%d"), edt.strftime("%Y%m%d"))
    st.dataframe(df_news)